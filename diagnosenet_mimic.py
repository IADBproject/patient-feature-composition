#################################################################################
#																				#
#	Towards a Green Intelligence Applied to Health Care and Well-Being			#
###-----------------------------------------------------------------------------#
# diagnosenet_datamining.py				                        				#
# Main to build a binary patient phenotype representation from DRG-PACA			#
#################################################################################


from __future__ import print_function
import os, sys
import numpy as np
import pandas as pd
import time
import logging

## dIAgnoseNET library
from diagnosenet.featurescomposition import FeaturesComposition
from diagnosenet.dtm import DocumentTermMatrix
from diagnosenet.vocabularycomposition import VocabularyComposition
from diagnosenet.labelcomposition import LabelComposition
from diagnosenet.logger import Config
logger = logging.getLogger('_dIAgnoseNET_DataMining')

class _dIAgnoseNET_DataMining:
	"""
	_dIAgnoseNet_DataMinig is a feature extraction library to build a
	general purpose patient phenotype in a binary representation
	from electronic health records.

	The implementation dataset is composed by diagnosis related group
	represented in object form; such as ICD-10 codes, CCAM codes and
	other codes established by the agency ATIH and generated by the system PMSI for
	Inpatients and Outpatients with records of hospitals activities in PACA and
	activities of residents PACA hospitalised in another region.

	* This class uses:
		+ diagnosenet_featurescomposition
		+ diagnosenet_vocabularycomposition
		+ diagnosenet_cdajson
		+ diagnosenet_labelcomposition

	* This class permit creates a dynamic features composition with
	their respective vocabulary to build a binary patient phenotype representation
	in a 'document-term sparse matrix' from Electronic Health Records (EHR) data.

	* This version implements the structured Clinical Documents Architecture (CDA)
	in a JSON format to mining the Hospital Data Warehouse (PMSI) in PACA.
	"""

	def __init__(self, *args, **kwargs):
		## dIAgnoseNet path variables
		self.dataset_dir = args[0]
		self.features_used = args[1]
		self.rawdata_name = args[2]
		self.sandbox = args[3]
		self.year = args[4]

		_dIAgnoseNET_DataMining.Dir_rawdata = str(self.dataset_dir+"/"+self.rawdata_name+"/")

	def displayDirectories(self):
		logger.info('-- Dataset Directory: {} --'.format(self.dataset_dir))
		logger.info('-- Directory with Raw Data: {} --'.format(self.Dir_rawdata))
		logger.info('-- Name EHR db: {} --'.format(self.features_used))
		logger.info('-- Sandbox directory: {} --'.format(self.sandbox))
		logger.info('-- Year {} --'.format(self.year))

	def get_tbl_header(self, filename):
		"""
		Get table header from cvs file
		"""
		return pd.read_csv(filename, index_col=0, nrows=0).columns.tolist()

	### ### PMSI-PACA ICU load files used at the Intensive Care Unit
	def load_and_clean_ICUData(self):
		load_time = time.time()
		
		raw_data = 'OUT_LIMIT_NUM_EVENTS_WINSIZE_24H.csv'
		filename = self.Dir_rawdata+self.year+"/"+str(raw_data)
		usecols = self.get_tbl_header(filename)
		icu_rsa = pd.read_csv(filename, usecols=usecols)

		# Clean data
		for (index_label, row_series) in icu_rsa.iterrows():
			icu_rsa.iloc[index_label,27] = self._clean_string(icu_rsa.iloc[index_label,27])
			icu_rsa.iloc[index_label,28] = self._clean_string(icu_rsa.iloc[index_label,28])
			icu_rsa.iloc[index_label,29] = self._clean_string(icu_rsa.iloc[index_label,29])
			icu_rsa.iloc[index_label,33] = self._clean_string(icu_rsa.iloc[index_label,33])

		icu_rsa.to_csv(self.Dir_rawdata+self.year+"/"+str(raw_data))

	### PMSI-PACA ICU load files used at the Intensive Care Unit
	def loadICUData(self):
		load_time = time.time()
		
		raw_data = 'OUT_LIMIT_NUM_EVENTS_WINSIZE_24H.csv'
		filename = self.Dir_rawdata+self.year+"/"+str(raw_data)
		usecols = self.get_tbl_header(filename)
		icu_rsa = pd.read_csv(self.Dir_rawdata+self.year+"/"+str(raw_data),
					header=0,sep=",",encoding='latin-1',dtype="str", usecols=usecols)

		### Merge SSR_fix with the new Primary Morbidty Composition 'Classes Disease Group'
		#;primary_Composition=pd.merge(icu_rsa, code_diagnosis, left_on='affect_etiol', right_on='Diagnose')
		logger.info('---------------------------------------------------------')
		logger.info('++ Load Data for ICU-{} ++'.format(str(self.year)))
		logger.info('-- SSR_FIX: {} --'.format(str(icu_rsa.shape)))
		# logger.info('-- ICD-10 Codes: {} --'.format(str(code_diagnosis.shape)))
		##logger.info('-- Primary Composition: %s ---"% str(primary_Composition.shape) )
		logger.debug('* Data load time: {} (SPY) *'.format((time.time() - load_time)))
		return icu_rsa

	def _clean_string(self, phrase):    
		if isinstance(phrase, str):
			from string import maketrans 
			chars_to_remove = " /``[]"
			replace_by = '-_    '
			trantab = maketrans(chars_to_remove, replace_by)
			phrase =  phrase.translate(trantab)

		return phrase


def main(argv):
	### Using dIAgnoseNet 'directories parameters' to Feature Extraction and Composition
	if len(argv) == 18:

		#######################################################################
		## Counter to measure the execution time of diagnosenet_datamining
		counter_execution = time.time()
		## Counter to measure the latency
		counter_latency = time.time()

		#######################################################################
		## Get Arguments from run_*.sh script
		dataset_dir = argv[0]
		features_name = argv[1]
		rawdata_name = argv[2]
		sandbox = argv[3]
		year = argv[4]
		argv_testbed = argv[5]
		x1_name = argv[6]
		x2_name = argv[7]
		x3_name = argv[8]
		x4_name = argv[9]
		x5_name = argv[10]
		x6_name = argv[11]
		x7_name = argv[12]
		x8_name = argv[13]
		x9_name = argv[14]
		x10_name = argv[15]
		medicaltarget = argv[16]
		vocabulary_type = argv[17]

		Dir_rawdata = str(dataset_dir+"/"+rawdata_name+"/")
		local_dir = "/1_Mining-Stage/"
		testbed_dir = "../enerGyPU/testbed/"

		#######################################################################
		### Set general logging configuration for using in _dIAgnoseNET_DataMining
	 	logging_config = Config()
		logging_config._setup_logger('_dIAgnoseNET_DataMining',
				str(sandbox+local_dir+'/diagnosenet_datamining.log'), logging.DEBUG)

		logger = logging.getLogger('_dIAgnoseNET_DataMining') ## Declare the same logging name
		logger.info('---------------------------------------------------------')
		logger.warning('!!! Using Parameters configured in run_*.sh !!!')
		logger.info('---------------------------------------------------------')

		#######################################################################
		### Starting  _dIAgnoseNET_DataMining and load row data
		datamining =  _dIAgnoseNET_DataMining(dataset_dir,features_name,rawdata_name,sandbox,year)
		datamining.displayDirectories()
		icu_rsa = datamining.loadICUData() ## Get the PMSI-PACA ICU Data

		## End counter to measure latency
		latency = time.time() - counter_latency
		logger.debug('* Latency: {} *'.format(latency))


		#######################################################################
		## Counter to Features Composition
		counter_fc = time.time()
		logger.info('---------------------------------------------------------')
		logger.info('++ Features Composition ++')

		### Features Serializer in a Clinical Document Architecture as JSON formet
		featurescomposition = FeaturesComposition(dataset_dir, features_name, sandbox, year)
		#featurescomposition._set_featuresSerializer(icu_rsa)
		#cda_object = featurescomposition._get_featuresSerialized(icu_rsa)

		## The CDA features serialization to write one time
		## Check if the clinical document exists
		cda_name = str(dataset_dir+'/CDA_Serialization/'+'clinical_document-'+year+'.json')
		if not os.path.exists(cda_name):
			featurescomposition._write_featuresSerialized(icu_rsa)
		cda_object = featurescomposition._read_featuresSerialized()
		logger.info('-- Number of CDA Records: [{}] --'.format(len(cda_object)))

		## End counter to Features Composition
		time_featurescomposition = time.time() - counter_fc
		logger.debug('* Features Composition Time: {} *'.format(time_featurescomposition))


		#######################################################################
		## Counter to Vocabulary Composition
		counter_vc = time.time()
		if vocabulary_type == 'custom':
			logger.info('---------------------------------------------------------')
			logger.info('++ Custom Vocabulary ++')

			### Read Custom Vocabulary
			vocabularycomposition = VocabularyComposition(dataset_dir, features_name, sandbox, year)
			voc_x1,voc_x2,voc_x3,voc_x4,voc_x5,voc_x6,voc_x7,voc_x8,voc_x9,voc_x10 = vocabularycomposition._custom_Vocabulary(x1_name.split(','),
										x2_name.split(','),x3_name.split(','),x4_name.split(','),
										x5_name.split(','),x6_name.split(','),x7_name.split(','),
										x8_name.split(','),x9_name.split(','),x10_name.split(','))
		### Static enviroment
		else:
			logger.info('---------------------------------------------------------')
			logger.info('++ Dynamic Vocabulary ++')

			## Buil a dynamic vocabulary
			vocabularycomposition = VocabularyComposition(dataset_dir, features_name, sandbox, year)
			voc_x1,voc_x2,voc_x3,voc_x4,voc_x5,voc_x6,voc_x7,voc_x8,voc_x9,voc_x10 = vocabularycomposition._dynamic_Vocabulary(cda_object,
										x1_name.split(','),x2_name.split(','),x3_name.split(','),
										x4_name.split(','),x5_name.split(','),x6_name.split(','),
										x7_name.split(','),x8_name.split(','),x9_name.split(','),x10_name.split(','))
			## Write dynamic vocabulary
			vocabularycomposition._write_Vocabulary(x1_name.split(','),x2_name.split(','),
								x3_name.split(','),x4_name.split(','),x5_name.split(','),x6_name.split(','),
								x7_name.split(','),x8_name.split(','),x9_name.split(','),x10_name.split(','),)

		## End Counter to label Composition
		time_vocabulary = time.time() - counter_vc
		logger.debug('* Vocabulary Composition Time: {} *'.format(time_vocabulary))


		#######################################################################
		## Counter to Binary representation include Vocabulary Load or Built
		counter_br = time.time()

		## Build a binary petient phenotype representation from their features selected
		dtm = DocumentTermMatrix(dataset_dir, features_name, sandbox, year, vocabulary_type)
		binarypatient = dtm._build_binaryPhenotype(cda_object, x1_name.split(','),
						x2_name.split(','), x3_name.split(','), x4_name.split(','),
						x5_name.split(','), x6_name.split(','), x7_name.split(','),
						x8_name.split(','), x9_name.split(','), x10_name.split(','),
						voc_x1,voc_x2,voc_x3,voc_x4,voc_x5,voc_x6,voc_x7,voc_x8,voc_x9,voc_x10)

		len_BPPR = dtm._write_binaryPhenotype()
		logger.info('-- Lenght of BPPR: [{}] --'.format(len_BPPR))

		## End Counter to Binary representation
		time_binaryrepresentation = time.time() - counter_br
		logger.debug('* Binary Representation Time: {} *'.format(time_binaryrepresentation))


		#######################################################################
		## Counter to label Composition
		counter_lc = time.time()

		### Label Composition
		label_composition = LabelComposition(features_name, sandbox, year)
		if medicaltarget == 'y1':
			logger.info('---------------------------------------------------------')
			logger.info('++ Medical Target Y1 ++')
			label_composition._get_PrimaryMorbidityLabel(cda_object)
			label_composition._build_BinaryPrimaryMorbidity()
			pm_lenght = label_composition._write_PrimaryMorbidityLabel()
			logger.info('-- Number of Y1 multilabels: [{}] --'.format(pm_lenght))

		elif medicaltarget == 'y2':
			logger.info('---------------------------------------------------------')
			logger.info('++ Medical Target Y2 ++')
			label_composition._set_clinicalProceduresVoc(cda_object)
			label_composition._build_clinicalProcedures(cda_object)
			cp_lenght = label_composition._write_clinicalProceduresLabel()
			logger.info('-- Number of Y2 multilabels: [{}] --'.format(cp_lenght))

		elif medicaltarget == 'y3':
			logger.info('---------------------------------------------------------')
			logger.info('++ Medical Target Y3 ++')
			label_composition._set_destinationVoc(cda_object)
			label_composition._build_Destination(cda_object)
			d_lenght = label_composition._write_DestinationLabel()
			logger.info('-- Number of Y3 multilabels: [{}] --'.format(d_lenght))
		else:
			logger.info('---------------------------------------------------------')
			logger.warning('!!! Medical target did not selected !!!')

		## End Counter to label Composition
		time_labelcomposition = time.time() - counter_lc
		logger.debug('* Label Composition Time: {} *'.format(time_labelcomposition))

		## End Counter to measure the application
		time_execution = time.time() - counter_execution
		logger.info('---------------------------------------------------------')
		logger.debug('* Execution Time: {} *'.format(time_execution))

		#######################################################################
		## Write the parameters used and time counters
		##time_counters=np.array([[latency],[time_featurescomposition],[time_vocabulary],
		##[time_binaryrepresentation],[time_labelcomposition],[time_execution]])
		##tesbed_file = str(testbed_dir+"/"+argv_testbed+"/"+argv_testbed+"-stage1_timecounters.csv")
		##np.savetxt(tesbed_file,time_counters.T, delimiter=',', fmt='%s')
		##logger.info('-- Testbed Directory: {} *'.format(tesbed_file))


	### Using default directory "healthData/sandbox-pre-trained"
	else:
		#######################################################################
		## Counter to measure the execution time of diagnosenet_datamining
		counter_execution = time.time()
		## Counter to measure the latency
		counter_latency = time.time()

		#######################################################################
		### Set Default Parameters
		dataset_dir = "healthData/"
		features_name = "pre-trained"
		rawdata_name = "MIMIC"
		sandbox = str("healthData/sandbox-")+str(features_name)
		year = "2019"
		Dir_rawdata = str(dataset_dir+"/"+rawdata_name+"/")
		local_dir = "/1_Mining-Stage/"
		#vocabulary_type = "custom"
		vocabulary_type = None
		##vocabulary_type = "None"
		medicaltarget = 'y1'

		#######################################################################
		### Select the features to build a binary patient phenotype
		### Demographical information
		x1_name = ['gender', 'age', 'marital_status', 'ethnicity']
		### Admission details
		x2_name = ['admission_type', 'admission_location', 'discharge_location', 'insurance', 'expire_flag']
		### Hospitalization details
		#x3_name = ['icu_first_careunit', 'icu_last_careunit', 'icu_los', 'procedure']
		x3_name = ['icu_first_careunit', 'icu_last_careunit', 'icu_los']
		# x4_name = ['dressing', 'feeding', 'displacement', 'continence' ]
		x4_name=None
		# x5_name = ['communication', 'comportement']
		x5_name=None
		# x6_name = ['mechanical_rehab', 'motorsensory_rehab', 'neuropsychological_rehab',
		#    		'cardiorespiratory_rehab', 'nutritional_rehab', 'urogenitalsphincter_rehab',
		#   		'kidneys_rehab', 'electrical_equipment', 'collective-rehab',
		#   		'bilans', 'physiotherapy', 'balneotherapy']
		x6_name=None
		##x7_name = ['x7_associated_diagnosis']
		x7_name = None
		#x7_name = ['procedure']
		##x8_name = ['care_purpose','morbidity','etiology','major_clinical_category']
		x8_name=None
		##x9_name = ['x9_clinical_procedures']
		x9_name=None
		#x10_name = ['last_week','output_mode','destination']
		x10_name=None

		#######################################################################
		### Set general logging configuration for using in _dIAgnoseNET_DataMining
		logging_config = Config()
                logging_config._setup_logger('_dIAgnoseNET_DataMining',
                                str(sandbox+local_dir+'/diagnosenet_datamining.log'), logging.DEBUG)

                logger = logging.getLogger('_dIAgnoseNET_DataMining') ## Declare the same logging name
                logger.info('---------------------------------------------------------')
                logger.warning('!!! Using Parameters configured in run_*.sh !!!')
                logger.info('---------------------------------------------------------')

		#######################################################################
		### Starting _dIAgnoseNET_DataMining and load row data
		dataminingDefault =  _dIAgnoseNET_DataMining(dataset_dir, features_name, rawdata_name, sandbox, year)
		dataminingDefault.displayDirectories()

		# Load and clean ICU data
		icu_rsa = dataminingDefault.load_and_clean_ICUData() ## Get the PMSI-PACA ICU Data
		icu_rsa = dataminingDefault.loadICUData() ## Get the PMSI-PACA ICU Data

		## End counter to measure latency
		latency = time.time() - counter_latency
		logger.debug('* Latency: {} *'.format(latency))

		#######################################################################
		## Counter to Features Composition
		counter_fc = time.time()
		logger.info('---------------------------------------------------------')
		logger.info('++ Features Composition ++')

		### Features Serializer in a Clinical Document Architecture as JSON formet
		featurescomposition = FeaturesComposition(dataset_dir, features_name, sandbox, year)
		#cda_object = featurescomposition._write_featuresSerialized(icu_rsa)
		cda_object = featurescomposition._get_featuresSerialized(icu_rsa)
		#cda_object = featurescomposition._set_featuresSerializer(icu_rsa)

		## The CDA features serialization to write one time
		## Check if the clinical document exists
		# cda_name = str(dataset_dir+'/CDA_Serialization/'+'clinical_document_mimic-'+year+'.json')
		# if not os.path.exists(cda_name):
		# 	featurescomposition._write_featuresSerialized(icu_rsa)
		# cda_object = featurescomposition._read_featuresSerialized()
		# logger.info('-- Number of CDA Records: [{}] --'.format(len(cda_object)))

		## End counter to Features Composition
		time_featurescomposition = time.time() - counter_fc
		logger.debug('* Features Composition Time: {} *'.format(time_featurescomposition))


		# #######################################################################
		# ## Counter to Vocabulary Composition
		counter_vc = time.time()
		if vocabulary_type == 'custom':
			logger.info('---------------------------------------------------------')
			logger.info('++ Custom Vocabulary ++')

			### Read Custom Vocabulary
			vocabularycomposition = VocabularyComposition(dataset_dir, features_name, sandbox, year)
			voc_x1,voc_x2,voc_x3,voc_x4,voc_x5,voc_x6,voc_x7,voc_x8,voc_x9,voc_x10 = vocabularycomposition._custom_Vocabulary(x1_name,
										x2_name,x3_name,x4_name, x5_name,x6_name, x7_name, x8_name, x9_name, x10_name)

		else:
			logger.info('---------------------------------------------------------')
			logger.info('++ Dynamic Vocabulary ++')

			## Buil a dynamic vocabulary
			vocabularycomposition = VocabularyComposition(dataset_dir, features_name, sandbox, year)
			voc_x1,voc_x2,voc_x3,voc_x4,voc_x5,voc_x6,voc_x7,voc_x8,voc_x9,voc_x10 = vocabularycomposition._dynamic_Vocabulary(cda_object,
										x1_name,x2_name,x3_name,x4_name,x5_name,x6_name,x7_name,x8_name,x9_name,x10_name)
			#print(vocabularycomposition)
			## Write dynamic vocabulary
			# vocabularycomposition._write_Vocabulary(x1_name,x2_name,x3_name,x4_name,x5_name,x6_name,x7_name,x8_name,x9_name,x10_name)
			vocabularycomposition._write_Vocabulary(x1_name,x2_name,x3_name,x4_name,x5_name,x6_name,x7_name,x8_name,x9_name,x10_name)

		## End Counter to label Composition
		time_vocabulary = time.time() - counter_vc
		logger.debug('* Vocabulary Composition Time: {} *'.format(time_vocabulary))

		# #######################################################################
		## Counter to Binary representation include Vocabulary Load or Built
		counter_br = time.time()

		## Build a binary petient phenotype representation using Document-term Matrix
		dtm = DocumentTermMatrix(dataset_dir, features_name, sandbox, year)
		binarypatient = dtm._build_binaryPhenotype(cda_object,x1_name,x2_name,x3_name,
						x4_name, x5_name, x6_name, x7_name, x8_name, x9_name, x10_name,
						voc_x1,voc_x2,voc_x3,voc_x4,voc_x5,voc_x6,voc_x7,voc_x8,voc_x9,voc_x10)

		len_BPPR = dtm._write_binaryPhenotype()
		logger.info('-- Lenght of BPPR: [{}] --'.format(len_BPPR))

		## End Counter to Binary representation
		time_binaryrepresentation = time.time() - counter_br
		logger.debug('* Binary Representation Time: {} *'.format(time_binaryrepresentation))


		# #######################################################################
		## Counter to label Composition
		# counter_lc = time.time()

		# ### Label Composition
		# label_composition = LabelComposition(features_name, sandbox, year)
		# if medicaltarget == 'y1':
		# 	logger.info('---------------------------------------------------------')
		# 	logger.info('++ Medical Target Y1 ++')
		# 	label_composition._get_PrimaryMorbidityLabel(cda_object)
		# 	label_composition._build_BinaryPrimaryMorbidity()
		# 	pm_lenght = label_composition._write_PrimaryMorbidityLabel()
		# 	logger.info('-- Number of Y1 multilabels: [{}] --'.format(pm_lenght))

		# elif medicaltarget == 'y2':
		# 	logger.info('---------------------------------------------------------')
		# 	logger.info('++ Medical Target Y2 ++')
		# 	label_composition._set_clinicalProceduresVoc(cda_object)
		# 	label_composition._build_clinicalProcedures(cda_object)
		# 	cp_lenght = label_composition._write_clinicalProceduresLabel()
		# 	logger.info('-- Number of Y2 multilabels: [{}] --'.format(cp_lenght))

		# elif medicaltarget == 'y3':
		# 	logger.info('---------------------------------------------------------')
		# 	logger.info('++ Medical Target Y3 ++')
		# 	label_composition._set_destinationVoc(cda_object)
		# 	label_composition._build_Destination(cda_object)
		# 	d_lenght = label_composition._write_DestinationLabel()
		# 	logger.info('-- Number of Y3 multilabels: [{}] --'.format(d_lenght))
		# else:
		# 	logger.info('---------------------------------------------------------')
		# 	logger.warning('!!! Medical target did not selected !!!')

		# ## End Counter to label Composition
		# time_labelcomposition = time.time() - counter_lc
		# logger.debug('* Label Composition Time: {} *'.format(time_labelcomposition))

		# ## End Counter to measure the application
		# time_execution = time.time() - counter_execution
		# logger.info('---------------------------------------------------------')
		# logger.debug('* Execution Time: {} *'.format(time_execution))

if __name__ == '__main__':
	main(sys.argv[1:])
